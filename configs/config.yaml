llm:
  #provider: "anthropic"
  provider: "ollama"
  
  # Ollama configuration
  ollama:
    url: "http://localhost:11434"
    model: "llama3.2"
    maxTokens: 1024

  # Anthropic configuration
  anthropic:
    model: "claude-3-5-haiku-latest"
    maxTokens: 1024
    # apiKey is typically set via environment variable

logger:
  verbose: false 